{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cf67b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import csv\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ffa9a",
   "metadata": {},
   "source": [
    "# Read Dataset\n",
    "read pain prediction dataset into classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed43c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = r\"C:\\Users\\ronny\\Documents\\GitHub\\MetaPain\\Data\\biosignals_filtered_Processed\\eda\"\n",
    "\n",
    "def get_pain_prediction(data_folder):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the dataset loaders for all tasks of pain prediction dataset\n",
    "    \"\"\"\n",
    "\n",
    "    files = os.listdir(data_folder)\n",
    "    num_tasks = len(files)\n",
    "    with open(data_folder+'/'+ files[0],'r') as f:\n",
    "        pickle_in = list(csv.reader(f, delimiter=','))\n",
    "        feature,target = np.array(pickle_in)[:,:-1].astype(float),np.array(pickle_in)[:,-1]\n",
    "        ind = ~np.isnan(feature).any(axis=1)\n",
    "        feature = feature[ind, :]\n",
    "        target = target[ind]\n",
    "        meta_ind = (target == \"PA1\")|(target == \"PA2\")|(target == \"PA3\")\n",
    "        local_ind = (target == \"BL1\")|(target == \"PA4\")\n",
    "        feature_local = feature[local_ind,:]\n",
    "        target_local = target[local_ind]\n",
    "        feature_meta = feature[meta_ind,:]\n",
    "        target_meta = target[meta_ind]\n",
    "        \n",
    "    for task_id in range(1,num_tasks):\n",
    "\n",
    "#         with open(os.path.join(data_folder, files[task_id-1]),'r') as f:\n",
    "        with open(data_folder+'/'+ files[task_id],'r') as f:\n",
    "\n",
    "            feature,target = np.array(pickle_in)[:,:-1].astype(float),np.array(pickle_in)[:,-1]\n",
    "            ind = ~np.isnan(feature).any(axis=1)\n",
    "            feature = feature[ind, :]\n",
    "            target = target[ind]\n",
    "            meta_ind = (target == \"PA1\")|(target == \"PA2\")|(target == \"PA3\")\n",
    "            local_ind = (target == \"BL1\")|(target == \"PA4\")\n",
    "            feature_local = np.concatenate((feature_local,feature[local_ind,:]))\n",
    "            target_local = np.concatenate((target_local,target[local_ind]))\n",
    "            feature_meta = np.concatenate((feature_meta,feature[meta_ind,:]))\n",
    "            target_meta = np.concatenate((target_meta,target[meta_ind]))\n",
    "            \n",
    "    feature_meta = normalize(torch.Tensor(feature_meta),dim=0)\n",
    "    feature_local = normalize(torch.Tensor(feature_local),dim=0)\n",
    "    target_meta = np.unique(target_meta, return_inverse=True)[1].astype(float)\n",
    "    target_local = np.unique(target_local, return_inverse=True)[1].astype(float)\n",
    "    dataset_local = TensorDataset(feature_local,torch.Tensor(target_local))\n",
    "    train_dataset_local, test_dataset_local = random_split(dataset_local, \n",
    "                                                           [int(len(target_local)*0.80), (len(target_local)-int(len(target_local)*0.80))],\n",
    "                                                           generator=torch.Generator().manual_seed(42))\n",
    "    dataset_meta = TensorDataset(feature_meta,torch.Tensor(target_meta))\n",
    "    train_dataset_meta, test_dataset_meta = random_split(dataset_meta, \n",
    "                                                           [int(len(target_meta)*0.80), (len(target_meta)-int(len(target_meta)*0.80))],\n",
    "                                                           generator=torch.Generator().manual_seed(42)) \n",
    "    datasets_local = {'train': train_dataset_local, 'test': test_dataset_local}\n",
    "    datasets_meta = {'train': train_dataset_meta, 'test': test_dataset_meta}\n",
    "\n",
    "    return datasets_local,datasets_meta\n",
    "\n",
    "datasets_local,datasets_meta = get_pain_prediction(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab3dab",
   "metadata": {},
   "source": [
    "# Network Construction\n",
    "\n",
    "Construct multilayer neural network for the pain prediction dataset with input dimension of 22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9612bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Two layer MLP for pain prediction dataset benchmarks.\n",
    "    \"\"\"\n",
    "    def __init__(self, hiddens,dropout):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(22, hiddens)\n",
    "        self.W2 = nn.Linear(hiddens, hiddens)\n",
    "        self.W3 = nn.Linear(hiddens, hiddens)\n",
    "        self.W4 = nn.Linear(hiddens, 2)\n",
    "        self.norm1 = nn.BatchNorm1d(hiddens)\n",
    "        self.norm2 = nn.BatchNorm1d(hiddens)\n",
    "        self.norm3 = nn.BatchNorm1d(hiddens)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "        self.dropout_3 = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.W1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.dropout_1(out)\n",
    "        out = self.W2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.dropout_2(out)\n",
    "        out = self.W3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.dropout_3(out)\n",
    "        out = self.W4(out)\n",
    "        \n",
    "        return out   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155b297",
   "metadata": {},
   "source": [
    "# Initiate meta training using PA1-PA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcfacf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18327820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loader, criterion):\n",
    "    \n",
    "    train_acc = AverageMeter()\n",
    "    train_loss = AverageMeter()\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "    for _,(data, target) in enumerate(loader):\n",
    "        data = data.to(DEVICE)\n",
    "        target = target.type(torch.LongTensor).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(data)\n",
    "        loss = criterion(pred.view(-1,2), target)\n",
    "        train_loss.update(loss.item())\n",
    "        \n",
    "        acc = (pred.argmax(dim=-1) == target).float().mean()\n",
    "        train_acc.update(acc.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return {'loss': train_loss.avg, 'Accuracy': train_acc.avg}\n",
    "\n",
    "        \n",
    "def eval_epoch(model, loader, criterion):\n",
    "    \n",
    "    test_acc = AverageMeter()\n",
    "    test_loss = AverageMeter()\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(DEVICE)\n",
    "            target = target.type(torch.LongTensor).to(DEVICE)\n",
    "            \n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, target)\n",
    "            test_loss.update(loss.item())\n",
    "            \n",
    "            acc = (pred.argmax(dim=-1) == target).float().mean()\n",
    "            test_acc.update(acc.item())\n",
    "            \n",
    "    return {'loss': test_loss.avg, 'Accuracy': test_acc.avg}\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe7e581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss is 0.6110656261444092, validation accuracy is 0.9474431818181818\n",
      "validation loss is 0.14755834571339868, validation accuracy is 0.9744318181818182\n",
      "validation loss is 0.04383430748500607, validation accuracy is 1.0\n",
      "validation loss is 0.019353784874758938, validation accuracy is 1.0\n",
      "validation loss is 0.009158547036349773, validation accuracy is 1.0\n",
      "validation loss is 0.004473472268066623, validation accuracy is 1.0\n",
      "validation loss is 0.002520426536317576, validation accuracy is 1.0\n",
      "validation loss is 0.001775235530327667, validation accuracy is 1.0\n",
      "validation loss is 0.005226738930849189, validation accuracy is 1.0\n",
      "validation loss is 0.0006065659584816207, validation accuracy is 1.0\n",
      "validation loss is 0.0007644741298546168, validation accuracy is 1.0\n",
      "validation loss is 0.00034577116523657673, validation accuracy is 1.0\n",
      "validation loss is 0.00019004443575712767, validation accuracy is 1.0\n",
      "validation loss is 0.00016187796295112506, validation accuracy is 1.0\n",
      "validation loss is 0.00011840325499757786, validation accuracy is 1.0\n",
      "validation loss is 0.00012678564631972802, validation accuracy is 1.0\n",
      "validation loss is 0.0001147145865781402, validation accuracy is 1.0\n",
      "validation loss is 9.489781454745257e-05, validation accuracy is 1.0\n",
      "validation loss is 0.00010466998487986116, validation accuracy is 1.0\n",
      "validation loss is 7.380995620306666e-05, validation accuracy is 1.0\n",
      "validation loss is 3.787486060570121e-05, validation accuracy is 1.0\n",
      "validation loss is 3.342283079374201e-05, validation accuracy is 1.0\n",
      "validation loss is 3.282753277744632e-05, validation accuracy is 1.0\n",
      "validation loss is 3.3275876789544284e-05, validation accuracy is 1.0\n",
      "validation loss is 2.8788782814940944e-05, validation accuracy is 1.0\n",
      "validation loss is 3.397147007821001e-05, validation accuracy is 1.0\n",
      "validation loss is 2.2395908341753636e-05, validation accuracy is 1.0\n",
      "validation loss is 4.520803221649575e-05, validation accuracy is 1.0\n",
      "validation loss is 1.4004776858200785e-05, validation accuracy is 1.0\n",
      "validation loss is 1.517961341464384e-05, validation accuracy is 1.0\n",
      "validation loss is 2.4287763153552078e-05, validation accuracy is 1.0\n",
      "validation loss is 1.54939391656874e-05, validation accuracy is 1.0\n",
      "validation loss is 1.2975224060276311e-05, validation accuracy is 1.0\n",
      "validation loss is 1.3927298492820807e-05, validation accuracy is 1.0\n",
      "validation loss is 1.620587681827601e-05, validation accuracy is 1.0\n",
      "validation loss is 1.1221263527907219e-05, validation accuracy is 1.0\n",
      "validation loss is 1.1392383550463075e-05, validation accuracy is 1.0\n",
      "validation loss is 1.3287999536260031e-05, validation accuracy is 1.0\n",
      "validation loss is 1.0361501153965946e-05, validation accuracy is 1.0\n",
      "validation loss is 1.0359763274953531e-05, validation accuracy is 1.0\n",
      "validation loss is 7.30495792760683e-06, validation accuracy is 1.0\n",
      "validation loss is 7.879965075765291e-06, validation accuracy is 1.0\n",
      "validation loss is 1.0273503324821252e-05, validation accuracy is 1.0\n",
      "validation loss is 8.918879178633109e-06, validation accuracy is 1.0\n",
      "validation loss is 5.925378197637408e-06, validation accuracy is 1.0\n",
      "validation loss is 6.5866076345793605e-06, validation accuracy is 1.0\n",
      "validation loss is 5.631701738324906e-06, validation accuracy is 1.0\n",
      "validation loss is 8.066651720252015e-06, validation accuracy is 1.0\n",
      "validation loss is 6.917078528865452e-06, validation accuracy is 1.0\n",
      "validation loss is 4.094812683516383e-06, validation accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(datasets_local['train'], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(datasets_local['test'], batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "model = MLP(64, 0.5)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_model = model.to(DEVICE)\n",
    "best_acc = 0\n",
    "epoch = 50\n",
    "\n",
    "for _ in np.arange(epoch):\n",
    "    \n",
    "    train_epoch(model, optimizer, train_loader, criterion)\n",
    "    metric = eval_epoch(model, test_loader, criterion)\n",
    "    \n",
    "    print(f\"validation loss is {metric['loss']}, validation accuracy is {metric['Accuracy']}\")\n",
    "    \n",
    "    if metric['Accuracy'] > best_acc:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_acc = metric['Accuracy']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a95795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
